
## 1. 앙상블 학습을 통한 분류의 개요

### 1) 앙상블 학습이란?
* 분류는 지도학습의 대표적인 유형. 기존 데이터가 어떤 레이블에 속하는지 패턴을 알고리즘으로 인지한 뒤 새롭게 관측된 데이터에 대한 레이블을 판별한다.

1. 나이브 베이즈 : Naive Bayes
2. 로지스틱 회귀 : Logistic Regression
3. 결정 트리 : Decision Tree
4. 서포트 벡터 머신 : SVM
5. 최소 근접 : KNN
6. 신경망 : ANN
7. 앙상블 : Ensemble

여기서 7번인 앙상블은 서로 다른, 또는 같은 머신러닝 알고리즘 결합을 의미한다.

***앙상블 학습*** 을 통한 분류란 여러 개의 분류기를 생성하고, 그 예측을 결합함으로써 보다 정확한 최종 예측을 도출하는 기법이다. 

Q. -> 이게 뭔뜻이냐?
A. 강력한 하나의 모델을 사용하는 대신 보다 약한 모델 여러 개를 조합하여 더 정확한 예측에 도움을 주는 방식

다양한 분류기의 예측결과를 결합했기 때문에 단일 분류기보다 높은 분류 예측 성능을 보인다.

다양한 관점을 가진 알고리즘이 서로 결합하여, 복잡한 현실 세계의 문제에 대하여 유연한 해결책을 제시한다는 것. ( 정형 데이터 분류와 회귀 분야에서 성능이 뛰어남 )

* 최종 예측 결과를 결정하는 방식에 따라 여러 유형이 있다.
  1) 보팅 : 각각의 다른 유형의 알고리즘 학습기가 투표를 통해 최종 예측 결과를 결정

  2) 배깅 : 같은 유형의 알고리즘 학습기가 각각 다른 train 데이터셋을 학습한 후 투표를 통해 최종 예측 결과를 결정

  3) 부스팅 

  4) 스태킹 : 각각의 다른 유형의 알고리즘 학습기가 예측한 결과값을 다시 train 데이터셋으로 만들어서 다른 모델로 재학습시켜 결과를 예측


 ### 2. 부스팅이란?

  여러 개의 약한 학습기가 순차적으로 학습-예측하는 과정에서, 잘못 예측한 데이터에 가중치를 부여함으로써 오류를 개선해나가는 학습 방식

  : 학습-예측을 끝낸 분류기가 자신이 잘못 예측한 오류 데이터에 대해서, 다음 순서에 오는 분류기는 이를 올바르게 예측할 수 있도록 해당 데이터에 가중치를 부여하는 과정이 반복 진행되는 것이다.

  [Boosting 머신러닝 알고리즘]
  * AdaBoost
  * GBM
  * XGBoost
  * LightGBM


  ## 2.GBM 개요

  ### 1)GBM이란?

  GBM을 알기 위해서는 먼저 AdaBoost에 대해 알아야한다.
  AdaBoost는 오류 데이터에 가중치를 부여하면서 부스팅을 수행하는 대표적 알고리즘.

  [그림 참고](https://dacon.io/competitions/official/235946/codeshare/5623)
  : 그림을 보면 더 이해가 감
  : 오류 데이터에 가중치를 준다는 것이 그림의 동그라미나 세모 중 잘못된 데이터의 크기를 키우는 것이라고 생각할 수 있음.


  GBM의 학습방법은 AdaBoost와 유사하지만, *경사하강법* 을 이용해 가중치를 업데이트 한다.

  * GBM은 CART (Classification and Regression Trees) 기반의 다른 알고리즘과 마찬가지로 분류뿐만 아니라 회귀도 수행할 수 있다. 

  * 여기서 분류와 회귀 차이!
  - 일단 분류와 회귀는 둘 다 독립변수와 종속변수가 있어야한다.
  - 종속변수가 숫자일 땐 회귀
  - 종속변수가 이름일 때 분류를 이용


  ### 2)GBM의 특징 : 경사하강법이란?

  경사 하강법은 경사/ 기울기를 줄여나감으로써 오류를 최소화하는 방법을 의미한다.

  실제값과 모델의 예측값 차이에 따른 오류값을 '잔차'라고 한다.

  이러한 잔차의 제곱합인 RSS를 비용함수 또는 손실 함수라고 부른다. 

  머신러닝 알고리즘은 이 비용 함수 값을 지속적으로 감소시켜 최종적으로 더 이상 감소하지 않는 최소의 오류값을 구하는 방향으로 학습이 이뤄진다.

  경사 하강법도 머신러닝 알고리즘의 학습과 유사

  손실 함수에 미분을 적용한 뒤 이 미분값이 계속 감소하는 방향으로 파라미터 w를 (Loss(w)) 조정하다가, 미분값이 더이상 감소하지 않는 지점을 비용함수가 최소인 지점으로 간주하고 그때의 파라미터 w를 반환하는 것이다.
  : 미분에서의 극솟/극댓점?

  경사=기울기=손실함수의 미분계수 
  이 값이 0이 되어야한다!!!!!!!!!!!1

  ### 3)GBM의 하이퍼 파라미터

  [GBM 하이퍼 파라미터]
  * loss: 경사 하강법에서 사용할 손실 함수를 지정
  * learning_rate : GBM이 학습을 진행할 때마다 적용하는 학습률. 약한 학습기가 순차적으로 오류값을 보정해 나가는데 적용하는 계수 . 0~1 사이의 값을 지정. 값이 작을수록 최소 오류값을 찾으므로 예측 성능이 높아질 수 있지만 수행시간이 오래 걸리고, 또 너무 작으면 모든 약한 학습기의 반복이 완료돼도 최소 오류 값을 찾지 못하는 경우가 발생. 반대로 값을 크게 설정하면 최소 오류값을 찾지 못해 예측 성능이 떨어질 수 있지만 빠른 수행이 가능. (기본값 0.1)

  * n_estimators: 약한 학습기의 개수.  많을 수록 예측 성능이 일정 수준까지는 좋아질 수 있지만 그만큼 수행시간이 오래 걸린다. (기본값 100)

  * subsample : 약한 학습기가 학습에 사용하는 데이터의 샘플링 비율로, 과적합이 우려되는 경우 1보다 작은 값으로 설정한다. 
  (기본값은 1, 즉 전체 학습 데이터를 기반으로 학습)

  * max_depth, max_features

  learning_rate는 n_estimators와 상호 보완적으로 조합해 사용한다.

  learning_rate를 작게 하고, n_estimators를 크게 하면 더이상 성능이 좋아지지 않는 한계점까지는 예측 성능이 좋아질 수 있다.

  ## 3.GBM 실습

  사이킷런에서 제공하는 ***GradientBoostingClassifier*** 클래스를 이용해 유방암 데이터에 대한 GBM 분류 실습을 진행한다.

  종양의 다양한 feature에 따라 앙성종양인지 양성종양인지 예측하기


  




